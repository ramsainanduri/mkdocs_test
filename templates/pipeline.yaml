#Basic Details of the pipeline
pipeline: 
  logo: /data/bnf/dev/ram/Pipelines/validation_reports/pipeline_documentation/templates/rs-logo-color.svg
  info: 
    name: Pipeline Documentation
    version: v1.1.0
    author: Ram Sai Nanduri
    author_email: Ram.Nanduri@skane.se
    git_repo: https://github.com/ramsainanduri/pipeline_documentation
    server_location: /data/bnf/dev/ram/Pipelines/validation_reports/pipeline_documentation
  introduction: |  
    This automatic documentation generation is a time-saving tool for developers and teams, as it eliminates the need to manually create and maintain documentation. It also helps ensure that documentation is up-to-date and consistent, as changes made to the pipeline.yaml file used for the document updation in a simple and easy way. 
  
    The pipeline.yaml file contains all the relevant information about the pipeline or tool, including the description, inputs, outputs, parameters, and usage. This information is used to generate the HTML and MD documents, which provide clear and detailed information about the pipeline or tool.
  
    The HTML document is visually appealing and easy to navigate, with links to different sections and a search bar for quickly finding specific information. The MD document is plain text, but can be formatted with Markdown syntax for a more readable and structured format. The MD document can be uploaded to a readthedocs server for online documentation. It uses the mkdocs format, with the required "docs" folder and related files in the project root folder. 
  
    Additionally, the generated documentation also includes a table of contents for easy navigation, and sections for examples. 
  
    Overall, this repo helps improve the documentation process for pipelines and tools, making it easier for others to understand and use them. 
  scope: #Can have few or all the sections, better to have all for all the pipelines to maintain uniformity
    purpose: |
      The program aims to simplify the process of creating and maintaining documentation for software projects using MkDocs and Read the Docs.
    audience: |
      The program is designed for developers, technical writers, and project managers who want to create high-quality documentation with minimal effort.
    functionality: |
      The program will automatically generate documentation for a software project using the MkDocs framework, which allows for the creation of user-friendly, responsive documentation sites. The documentation site will be hosted on Read the Docs, which provides a robust platform for hosting and managing documentation.
    features: |
      The program will include the following features:
      - Automated creation of documentation using MkDocs
      - Integration with Read the Docs for hosting and management of documentation sites
      - Customizable templates for documentation sites
      - Support for multiple documentation versions and languages
      - Integration with source control systems such as Git
    technology_stack: |
      The program will be developed using Python and will use the MkDocs and Read the Docs APIs to generate and host documentation sites.
    limitations: |
      The program may be limited by the capabilities of MkDocs and Read the Docs, and may not be suitable for projects with complex documentation requirements.
    maintenance_and_support: |
      The program will be maintained and supported by the development team, who will provide regular updates and bug fixes. Documentation and user support will also be provided.
    expected_outcomes: |
      The program is expected to simplify the process of creating and maintaining documentation, reduce the amount of time and effort required to create documentation sites, and improve the overall quality of documentation for software projects.
  input_data: #mandatory keys
    input_desc: | 
      The input data for the pipeline consists of fastq files. However, for the pipeline to consume the data, it needs to be provided in the form of a CSV
      file that includes metadata. Below is an example of the CSV file format that is expected, along with a detailed description of each column.
    input_csv: /data/bnf/dev/ram/Pipelines/validation_reports/pipeline_documentation/templates/template_input.csv
    column_descriptions: #feel free to change the keys below depending the columns in your pipeline csv input file
      sample_id: |
        Text representating the name or id of the sample being analysed
      type: |
        Type of the sample, eg. tumor or normal
      assay: |
        Assay of the sample, eg. tumorWGS, myeloid, solidtumor etc
      platform: |
        Name of the paltform used for sequencing, eg. illumina
      read1: |
        Full path to the read 1 fastq file
      read2: |
        Full path to the read 2 fastq file
  output_data: 
    output_desc: |  
      This pipeline spits out various files from different process, the important once are given below with a brief descripton 
    output_files: #feel free to change the keys below depending the output files for your pipeline
      BAM: |
        A BAM file is a compressed binary file format used to store and index high-throughput sequencing data, such as DNA sequence reads aligned to a reference genome
      VCF: |
        A VCF file is a text file format used to store and annotate genetic variation data, such as single nucleotide polymorphisms (SNPs) and small insertions/deletions (indels), identified from sequencing data.
      HTML_Report: |
        An HTML documentation report is a text-based file format used to present information in a web browser, including text, images, and hyperlinks, typically used for displaying project documentation and results
      Markdown_Files: |
        A Markdown file is a lightweight markup language used to format and structure plain text documents, often used for creating documentation, README files, and notes
  min_requirements: # Can be multiple requirements but only text values are accepted
    configs: |
      pipeline.yaml, list_of_softwares.tsv  
    os: "POSIX based"
    cpus: 1
    singularity: ">= 3.8.0"
  usage: |
    To generate offline HTML documentation and an online Markdown(MD) file in your project folder, clone the repo and execute the bash script bin/runMe.sh.  

    To learn how to parse parameters for the script, run, 
    
    `bin/runMe.sh -h`

    To create an example script using the provided template files in the repo location, run,

    `bin/runMe.sh -e`
  pipeline_components:  #feel free to change the keys below depending the analysis steps for your pipeline
    pipeline_components_desc: | #Mandatory
      To automate the documentation for a pipeline, the following pipeline components are included, #Can have specific keys based on the pipeline, free free to modify, add, delete keys in this section depending on the pipeline
    data_retrieval: |
      This step involves retrieving data related to the pipeline, such as the code, input data, and output data.
    parsing: |
      This step involves parsing the code and the input and output data to extract relevant information, such as the pipeline components, their parameters, and their inputs and outputs.
    template_generation: |
      This step involves generating a template for the documentation based on the parsed information. The template should include sections for the pipeline components, their descriptions, their parameters, and their inputs and outputs.
    documentation_generation: |
      This step involves generating the actual documentation by populating the template with the parsed information. The documentation should be generated in a format that is easily readable and accessible, such as HTML or PDF.
    version_control: |
      This step involves using version control software, such as Git, to track changes to the documentation over time, and to maintain a history of the documentation.
    workflow_management: |
      This step involves using workflow management software, such as Snakemake or Nextflow, to automate the pipeline components and manage dependencies between the components.
    maintenance_and_support: |
      This step involves maintaining and supporting the documentation over time, including regular updates and bug fixes.
  software_stack: /data/bnf/dev/ram/Pipelines/validation_reports/pipeline_documentation/templates/template_tool_versions.yaml
  profiles: #Can be multiple profiles with different names
    profile1:
      profile_name: 'Profile 1'
      profile_description: 'Profile 1 is used for Solid Panel'
      profile_usage: 'add --profile "solid" to the nextflow command'
      profile_validatation_data_path: '/full/path/to/profile1/validation/data'
      profile_test_data_path: '/full/path/to/profile1/test/data'
    profile2:
      profile_name: 'Profile 2'
      profile_description: 'Profile 1 is used for AML Panel'
      profile_usage: 'add --profile "myeloid" to the nextflow command'
      profile_validatation_data_path: '/full/path/to/profile2/validation/data'
      profile_test_data_path: '/full/path/to/profile2/test/data'
  workflow: 'https://raw.githubusercontent.com/ramsainanduri/pipeline_documentation/dev/templates/template_workflow.png' 

